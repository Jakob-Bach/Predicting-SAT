\documentclass{article}

\title{Predicting Satisfiability of Benchmark Instances}
\author{Jakob Bach \and Markus Iser}

\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage{graphicx} % plots
\usepackage{hyperref} % links and URLs
\addbibresource{references.bib}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

SATzilla~\cite{xu2008satzilla, xu2012satzilla2012}, ISAC~\cite{kadioglu2010isac}, SNNAP~\cite{collautti2013snnap}
ML interpretability~\cite{carvalho2019machine, gilpin2018explaining}

\paragraph{Contributions}

\paragraph{Results}

\paragraph{Outline}

Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:experimental-design} introduces our experimental design.
Section~\ref{sec:evaluation} presents experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related-work}

- predict satisfiability with different instance sets~\cite{devlin2008satisfiability}
- predict satisfiability with different instance sets and feature selection~\cite{xu2007hierarchical}
  - part of Satzilla 2007~\cite{xu2007satzilla} / 2009~\cite{xu2008satzilla}
- feature extractor and evaluation with sat + category prediction~\cite{provan2022satfeatpy}
- predict satisfiability of 3SAT instances generated to be in a critical region (boundary SAT/UNSAT)~\cite{xu2012predicting}
- another paper working with uniform 3SAT instances~\cite{leyton2014understanding}

- graph neural network to predict SAT for generated instances~\cite{bunz2017graph}
- neural networks to predict satisfiability, working with uniform random 3 SAT instances close to phase transition (like Xu et al. 2012)~\cite{cameron2020predicting}
- another, heavily cited neural network approach (can also help to find satisfying assignment)~\cite{selsam2019learning}
- neural network working with classical feature representation~\cite{danisovszky2020classification}
- auto-encoder to predict satisfiability~\cite{dalla2021automated}
- very recent NN approach addressing problem of decreased prediction quality if test formulas are smaller than train formulas~\cite{zhang2022towards}
- predicting satisfiability of formulas with some other special structure~\cite{elhalaby2020learning}

- related topic: do not predict SAT itself, but use ML to guide solvers~\cite{amizadeh2019pdp, soos2019crystal, selsam2019guiding, zhang2021nlocalsat} or analyze performance of solvers~\cite{hutter2013identifying} or select solver~\cite{xu2012satzilla2012, bach2022comprehensive} or configure solver~\cite{hoos2021automated}
- also: predict SAT to guide solver~\cite{wu2017improving}
- also: predict properties other than satisfiability, e.g., temperature~\cite{giraldez2021temperature}, family~\cite{ansotegui2017structure, li2021hierarchical}, or hardness (runtime)~\cite{nudelman2004understanding, xu2007hierarchical, li2021hierarchical}

- related topic: predict satisfiability of other problems, e.g., CSPs~\cite{xu2018towards, liu2020learning, toenshoff2021graph}

- surveys on ML for SAT and other problems~\cite{amrani2018ml, popescu2022overview, guo2022machine} (also address predicting SAT)

- recent survey on structural features of SAT instances: \cite{alyahya2022structure}

- possibly interesting perspective what we can learn at all~\cite{yehuda2020its}

\section{Experimental Design}
\label{sec:experimental-design}

\subsection{Datasets}

3 instance sets:
- SAT Competition 2020~\cite{balyo2020proceedings} main (400)
- SAT Competition 2021~\cite{balyo2021proceedings} main (400)
- `Solved': all instances from `meta' with results known (11159)

Ratio of SAT / UNSAT roughly balanced in Sc 2021 and `Solved', SC datasets also contain unknowns (3rd class in prediction)
Distribution of instance families varies between datasets; SC datasets show more even distribution
Distribution of satisfiability varies greatly between families; we do not use family information directly as feature, but classifiers might leverage properties of certain families indirectly (if families imply particular structure of instances)

Datasets may contain missing values. -> describe

2 feature sets:
- SATzilla~\cite{xu2012features} (138)
- base (56) + gate (57)

Should mention number of constant / nearly constant features.

\subsection{Predictions}

Pre-processing: mean imputation, standard scaling

Mixture of simple and complex models
- simple: kNN, decision tree~\cite{breiman1984classification}
- complex: random forests~\cite{breiman2001random} and XGBoost~\cite{chen2016xgboost}

Matthews Correlation Coefficient (MCC)~\cite{matthews1975comparison, gorodkin2004comparing}

10-fold stratified CV

filter feature selection: None, 1, 2, 3, 4, 5, 10, 20

Analysis of feature importance: filter~\cite{guyon2003introduction} scores (to be concrete: MI~\cite{kraskov2004estimating}), model importance, (average normalized absolute) SHAP~\cite{lundberg2017unified}

\subsection{Implementation}

\emph{scikit-learn} \cite{pedregosa2011scikit}, \emph{gbd-tools}~\cite{iser2021collaborative}

\section{Evaluation}
\label{sec:evaluation}

Maybe describe feature distribution/outliers?
Correlation/MI of features to target
Should also mention correlation/MI between features (maybe remove high correlation beforehand?)
Could also include results of PCA (how many variance explained by how many components)

Analysis of misclassified instances: correlation of feature values to misclassification; misclassification ratio for families

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

\subsection{Future Work}

\printbibliography

\end{document}
